{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a83355",
   "metadata": {
    "id": "63a83355"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8c8e8",
   "metadata": {
    "id": "86e8c8e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import OrderedDict, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2d6c2",
   "metadata": {
    "id": "f0a2d6c2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfce8fb",
   "metadata": {
    "id": "dbfce8fb"
   },
   "outputs": [],
   "source": [
    "from mmseg.registry import DATASETS, TRANSFORMS, MODELS, METRICS\n",
    "from mmseg.datasets import BaseSegDataset\n",
    "from mmseg.models.segmentors import EncoderDecoder\n",
    "from mmseg.models.decode_heads import ASPPHead, FCNHead, SegformerHead\n",
    "from mmseg.models.utils.wrappers import resize\n",
    "\n",
    "\n",
    "from mmengine.config import Config\n",
    "from mmengine.dataset import Compose\n",
    "from mmengine.runner import Runner, load_checkpoint\n",
    "from mmengine.evaluator import BaseMetric\n",
    "from mmengine.logging import MMLogger, print_log\n",
    "from mmengine.structures import PixelData\n",
    "\n",
    "from mmcv.transforms import BaseTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdfd8a",
   "metadata": {
    "id": "b9bdfd8a"
   },
   "outputs": [],
   "source": [
    "# 데이터 경로를 입력하세요\n",
    "\n",
    "IMAGE_ROOT = \"/data/ephemeral/home/data/train/DCM/\"\n",
    "LABEL_ROOT = \"/data/ephemeral/home/data/train/outputs_json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ec100",
   "metadata": {
    "id": "c10ec100"
   },
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'finger-1', 'finger-2', 'finger-3', 'finger-4', 'finger-5',\n",
    "    'finger-6', 'finger-7', 'finger-8', 'finger-9', 'finger-10',\n",
    "    'finger-11', 'finger-12', 'finger-13', 'finger-14', 'finger-15',\n",
    "    'finger-16', 'finger-17', 'finger-18', 'finger-19', 'Trapezium',\n",
    "    'Trapezoid', 'Capitate', 'Hamate', 'Scaphoid', 'Lunate',\n",
    "    'Triquetrum', 'Pisiform', 'Radius', 'Ulna',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d4d86",
   "metadata": {
    "id": "808d4d86"
   },
   "outputs": [],
   "source": [
    "CLASS2IND = {v: i for i, v in enumerate(CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f20766",
   "metadata": {
    "id": "14f20766"
   },
   "outputs": [],
   "source": [
    "IND2CLASS = {v: k for k, v in CLASS2IND.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41dcbce",
   "metadata": {
    "id": "e41dcbce"
   },
   "outputs": [],
   "source": [
    "pngs = {\n",
    "    os.path.relpath(os.path.join(root, fname), start=IMAGE_ROOT)\n",
    "    for root, _dirs, files in os.walk(IMAGE_ROOT)\n",
    "    for fname in files\n",
    "    if os.path.splitext(fname)[1].lower() == \".png\"\n",
    "}\n",
    "\n",
    "jsons = {\n",
    "    os.path.relpath(os.path.join(root, fname), start=LABEL_ROOT)\n",
    "    for root, _dirs, files in os.walk(LABEL_ROOT)\n",
    "    for fname in files\n",
    "    if os.path.splitext(fname)[1].lower() == \".json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148c927",
   "metadata": {
    "id": "8148c927"
   },
   "outputs": [],
   "source": [
    "pngs = sorted(pngs)\n",
    "jsons = sorted(jsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57297cf",
   "metadata": {
    "id": "b57297cf"
   },
   "source": [
    "# MixIn Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c83fab",
   "metadata": {
    "id": "c2c83fab"
   },
   "outputs": [],
   "source": [
    "class PostProcessResultMixin:\n",
    "    def postprocess_result(self,\n",
    "                           seg_logits,\n",
    "                           data_samples):\n",
    "        \"\"\" Convert results list to `SegDataSample`.\n",
    "        Args:\n",
    "            seg_logits (Tensor): The segmentation results, seg_logits from\n",
    "                model of each input image.\n",
    "            data_samples (list[:obj:`SegDataSample`]): The seg data samples.\n",
    "                It usually includes information such as `metainfo` and\n",
    "                `gt_sem_seg`. Default to None.\n",
    "        Returns:\n",
    "            list[:obj:`SegDataSample`]: Segmentation results of the\n",
    "            input images. Each SegDataSample usually contain:\n",
    "\n",
    "            - ``pred_sem_seg``(PixelData): Prediction of semantic segmentation.\n",
    "            - ``seg_logits``(PixelData): Predicted logits of semantic\n",
    "                segmentation before normalization.\n",
    "        \"\"\"\n",
    "        batch_size, C, H, W = seg_logits.shape\n",
    "\n",
    "        if data_samples is None:\n",
    "            data_samples = [SegDataSample() for _ in range(batch_size)]\n",
    "            only_prediction = True\n",
    "        else:\n",
    "            only_prediction = False\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if not only_prediction:\n",
    "                img_meta = data_samples[i].metainfo\n",
    "                # remove padding area\n",
    "                if 'img_padding_size' not in img_meta:\n",
    "                    padding_size = img_meta.get('padding_size', [0] * 4)\n",
    "                else:\n",
    "                    padding_size = img_meta['img_padding_size']\n",
    "                padding_left, padding_right, padding_top, padding_bottom =\\\n",
    "                    padding_size\n",
    "                # i_seg_logits shape is 1, C, H, W after remove padding\n",
    "                i_seg_logits = seg_logits[i:i + 1, :,\n",
    "                                          padding_top:H - padding_bottom,\n",
    "                                          padding_left:W - padding_right]\n",
    "\n",
    "                flip = img_meta.get('flip', None)\n",
    "                if flip:\n",
    "                    flip_direction = img_meta.get('flip_direction', None)\n",
    "                    assert flip_direction in ['horizontal', 'vertical']\n",
    "                    if flip_direction == 'horizontal':\n",
    "                        i_seg_logits = i_seg_logits.flip(dims=(3, ))\n",
    "                    else:\n",
    "                        i_seg_logits = i_seg_logits.flip(dims=(2, ))\n",
    "\n",
    "                # resize as original shape\n",
    "                i_seg_logits = resize(\n",
    "                    i_seg_logits,\n",
    "                    size=img_meta['ori_shape'],\n",
    "                    mode='bilinear',\n",
    "                    align_corners=self.align_corners,\n",
    "                    warning=False).squeeze(0)\n",
    "            else:\n",
    "                i_seg_logits = seg_logits[i]\n",
    "\n",
    "            i_seg_logits = i_seg_logits.sigmoid()\n",
    "            i_seg_pred = (i_seg_logits > 0.5).to(i_seg_logits)\n",
    "\n",
    "            data_samples[i].set_data({\n",
    "                'seg_logits':\n",
    "                PixelData(**{'data': i_seg_logits}),\n",
    "                'pred_sem_seg':\n",
    "                PixelData(**{'data': i_seg_pred})\n",
    "            })\n",
    "\n",
    "        return data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e3009",
   "metadata": {
    "id": "ad9e3009"
   },
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class EncoderDecoderWithoutArgmax(PostProcessResultMixin, EncoderDecoder):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a42dfc",
   "metadata": {
    "id": "50a42dfc"
   },
   "outputs": [],
   "source": [
    "class LossByFeatMixIn:\n",
    "    def loss_by_feat(self, seg_logits, batch_data_samples) -> dict:\n",
    "        \"\"\"Compute segmentation loss.\n",
    "\n",
    "        Args:\n",
    "            seg_logits (Tensor): The output from decode head forward function.\n",
    "            batch_data_samples (List[:obj:`SegDataSample`]): The seg\n",
    "                data samples. It usually includes information such\n",
    "                as `metainfo` and `gt_sem_seg`.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, Tensor]: a dictionary of loss components\n",
    "        \"\"\"\n",
    "\n",
    "        seg_label = self._stack_batch_gt(batch_data_samples)\n",
    "        loss = dict()\n",
    "        seg_logits = resize(\n",
    "            input=seg_logits,\n",
    "            size=seg_label.shape[2:],\n",
    "            mode='bilinear',\n",
    "            align_corners=self.align_corners)\n",
    "        if self.sampler is not None:\n",
    "            seg_weight = self.sampler.sample(seg_logits, seg_label)\n",
    "        else:\n",
    "            seg_weight = None\n",
    "\n",
    "        if not isinstance(self.loss_decode, nn.ModuleList):\n",
    "            losses_decode = [self.loss_decode]\n",
    "        else:\n",
    "            losses_decode = self.loss_decode\n",
    "        for loss_decode in losses_decode:\n",
    "            if loss_decode.loss_name not in loss:\n",
    "                loss[loss_decode.loss_name] = loss_decode(\n",
    "                    seg_logits,\n",
    "                    seg_label,\n",
    "                    weight=seg_weight,\n",
    "                    ignore_index=self.ignore_index)\n",
    "            else:\n",
    "                loss[loss_decode.loss_name] += loss_decode(\n",
    "                    seg_logits,\n",
    "                    seg_label,\n",
    "                    weight=seg_weight,\n",
    "                    ignore_index=self.ignore_index)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0184d4",
   "metadata": {
    "id": "2d0184d4"
   },
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class ASPPHeadWithoutAccuracy(LossByFeatMixIn, ASPPHead):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289137f",
   "metadata": {
    "id": "c289137f"
   },
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class FCNHeadWithoutAccuracy(LossByFeatMixIn, FCNHead):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727ee93",
   "metadata": {
    "id": "c727ee93"
   },
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class SegformerHeadWithoutAccuracy(LossByFeatMixIn, SegformerHead):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd52579",
   "metadata": {
    "id": "bdd52579"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c20f11",
   "metadata": {
    "id": "f6c20f11"
   },
   "outputs": [],
   "source": [
    "@DATASETS.register_module()\n",
    "class XRayDataset(BaseSegDataset):\n",
    "    def __init__(self, is_train, **kwargs):\n",
    "        self.is_train = is_train\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def load_data_list(self):\n",
    "        _filenames = np.array(pngs)\n",
    "        _labelnames = np.array(jsons)\n",
    "\n",
    "        # split train-valid\n",
    "        # 한 폴더 안에 한 인물의 양손에 대한 `.dcm` 파일이 존재하기 때문에\n",
    "        # 폴더 이름을 그룹으로 해서 GroupKFold를 수행합니다.\n",
    "        # 동일 인물의 손이 train, valid에 따로 들어가는 것을 방지합니다.\n",
    "        groups = [os.path.dirname(fname) for fname in _filenames]\n",
    "\n",
    "        # dummy label\n",
    "        ys = [0 for fname in _filenames]\n",
    "\n",
    "        # 전체 데이터의 20%를 validation data로 쓰기 위해 `n_splits`를\n",
    "        # 5으로 설정하여 KFold를 수행합니다.\n",
    "        gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "        filenames = []\n",
    "        labelnames = []\n",
    "        for i, (x, y) in enumerate(gkf.split(_filenames, ys, groups)):\n",
    "            if self.is_train:\n",
    "                # 0번을 validation dataset으로 사용합니다.\n",
    "                if i == 0:\n",
    "                    continue\n",
    "\n",
    "                filenames += list(_filenames[y])\n",
    "                labelnames += list(_labelnames[y])\n",
    "\n",
    "            else:\n",
    "                filenames = list(_filenames[y])\n",
    "                labelnames = list(_labelnames[y])\n",
    "\n",
    "                # skip i > 0\n",
    "                break\n",
    "\n",
    "        data_list = []\n",
    "        for i, (img_path, ann_path) in enumerate(zip(filenames, labelnames)):\n",
    "            data_info = dict(\n",
    "                img_path=os.path.join(IMAGE_ROOT, img_path),\n",
    "                seg_map_path=os.path.join(LABEL_ROOT, ann_path),\n",
    "            )\n",
    "            data_list.append(data_info)\n",
    "\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc590843",
   "metadata": {
    "id": "dc590843"
   },
   "outputs": [],
   "source": [
    "@TRANSFORMS.register_module()\n",
    "class LoadXRayAnnotations(BaseTransform):\n",
    "    def transform(self, result):\n",
    "        label_path = result[\"seg_map_path\"]\n",
    "\n",
    "        image_size = (2048, 2048)\n",
    "\n",
    "        # process a label of shape (H, W, NC)\n",
    "        label_shape = image_size + (len(CLASSES), )\n",
    "        label = np.zeros(label_shape, dtype=np.uint8)\n",
    "\n",
    "        # read label file\n",
    "        with open(label_path, \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "        annotations = annotations[\"annotations\"]\n",
    "\n",
    "        # iterate each class\n",
    "        for ann in annotations:\n",
    "            c = ann[\"label\"]\n",
    "            class_ind = CLASS2IND[c]\n",
    "            points = np.array(ann[\"points\"])\n",
    "\n",
    "            # polygon to mask\n",
    "            class_label = np.zeros(image_size, dtype=np.uint8)\n",
    "            cv2.fillPoly(class_label, [points], 1)\n",
    "            label[..., class_ind] = class_label\n",
    "\n",
    "        result[\"gt_seg_map\"] = label\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac2bf5",
   "metadata": {
    "id": "c2ac2bf5"
   },
   "outputs": [],
   "source": [
    "@TRANSFORMS.register_module()\n",
    "class TransposeAnnotations(BaseTransform):\n",
    "    def transform(self, result):\n",
    "        result[\"gt_seg_map\"] = np.transpose(result[\"gt_seg_map\"], (2, 0, 1))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f47c5",
   "metadata": {
    "id": "1a4f47c5"
   },
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d228a51",
   "metadata": {
    "id": "8d228a51"
   },
   "outputs": [],
   "source": [
    "@METRICS.register_module()\n",
    "class DiceMetric(BaseMetric):\n",
    "    def __init__(self,\n",
    "                 collect_device='cpu',\n",
    "                 prefix=None,\n",
    "                 **kwargs):\n",
    "        super().__init__(collect_device=collect_device, prefix=prefix)\n",
    "\n",
    "    @staticmethod\n",
    "    def dice_coef(y_true, y_pred):\n",
    "        y_true_f = y_true.flatten(-2)\n",
    "        y_pred_f = y_pred.flatten(-2)\n",
    "        intersection = torch.sum(y_true_f * y_pred_f, -1)\n",
    "\n",
    "        eps = 0.0001\n",
    "        return (2. * intersection + eps) / (torch.sum(y_true_f, -1) + torch.sum(y_pred_f, -1) + eps)\n",
    "\n",
    "    def process(self, data_batch, data_samples):\n",
    "        \"\"\"Process one batch of data and data_samples.\n",
    "\n",
    "        The processed results should be stored in ``self.results``, which will\n",
    "        be used to compute the metrics when all batches have been processed.\n",
    "\n",
    "        Args:\n",
    "            data_batch (dict): A batch of data from the dataloader.\n",
    "            data_samples (Sequence[dict]): A batch of outputs from the model.\n",
    "        \"\"\"\n",
    "        for data_sample in data_samples:\n",
    "            pred_label = data_sample['pred_sem_seg']['data']\n",
    "\n",
    "            label = data_sample['gt_sem_seg']['data'].to(pred_label)\n",
    "            self.results.append(\n",
    "                self.dice_coef(label, pred_label)\n",
    "            )\n",
    "\n",
    "    def compute_metrics(self, results):\n",
    "        \"\"\"Compute the metrics from processed results.\n",
    "\n",
    "        Args:\n",
    "            results (list): The processed results of each batch.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: The computed metrics. The keys are the names of\n",
    "                the metrics, and the values are corresponding results.\n",
    "        \"\"\"\n",
    "        logger: MMLogger = MMLogger.get_current_instance()\n",
    "\n",
    "        results = torch.stack(self.results, 0)\n",
    "        dices_per_class = torch.mean(results, 0)\n",
    "        avg_dice = torch.mean(dices_per_class)\n",
    "\n",
    "        ret_metrics = {\n",
    "            \"Dice\": dices_per_class.detach().cpu().numpy(),\n",
    "        }\n",
    "        # summary table\n",
    "        ret_metrics_summary = OrderedDict({\n",
    "            ret_metric: np.round(np.nanmean(ret_metric_value) * 100, 2)\n",
    "            for ret_metric, ret_metric_value in ret_metrics.items()\n",
    "        })\n",
    "\n",
    "        metrics = {\n",
    "            \"mDice\": torch.mean(dices_per_class).item()\n",
    "        }\n",
    "\n",
    "        # each class table\n",
    "        ret_metrics.pop('aAcc', None)\n",
    "        ret_metrics_class = OrderedDict({\n",
    "            ret_metric: np.round(ret_metric_value * 100, 2)\n",
    "            for ret_metric, ret_metric_value in ret_metrics.items()\n",
    "        })\n",
    "        ret_metrics_class.update({'Class': CLASSES})\n",
    "        ret_metrics_class.move_to_end('Class', last=False)\n",
    "        class_table_data = PrettyTable()\n",
    "        for key, val in ret_metrics_class.items():\n",
    "            class_table_data.add_column(key, val)\n",
    "\n",
    "        print_log('per class results:', logger)\n",
    "        print_log('\\n' + class_table_data.get_string(), logger=logger)\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380e2d8",
   "metadata": {
    "id": "1380e2d8"
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17734392",
   "metadata": {
    "id": "17734392",
    "outputId": "b57be5c0-bf34-433f-9b71-9f88ed324efc"
   },
   "outputs": [],
   "source": [
    "%%writefile dataset_setting.py\n",
    "\n",
    "# dataset settings\n",
    "dataset_type = 'XRayDataset'\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadXRayAnnotations'),\n",
    "    dict(type='Resize', scale=(512, 512)),\n",
    "    dict(type='TransposeAnnotations'),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "val_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='Resize', scale=(512, 512)),\n",
    "    # add loading annotation after ``Resize`` because ground truth\n",
    "    # does not need to do resize data transform\n",
    "    dict(type='LoadXRayAnnotations'),\n",
    "    dict(type='TransposeAnnotations'),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='Resize', scale=(512, 512)),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(type='InfiniteSampler', shuffle=True),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        is_train=True,\n",
    "        pipeline=train_pipeline\n",
    "    )\n",
    ")\n",
    "val_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        is_train=False,\n",
    "        pipeline=val_pipeline\n",
    "    )\n",
    ")\n",
    "test_dataloader = val_dataloader\n",
    "\n",
    "val_evaluator = dict(type='DiceMetric')\n",
    "test_evaluator = val_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4455f777",
   "metadata": {
    "id": "4455f777"
   },
   "outputs": [],
   "source": [
    "%%writefile config_for_this_example.py\n",
    "\n",
    "# Train Segformer Mit B3\n",
    "_base_ = [\n",
    "    \"mmsegmentation/configs/_base_/models/segformer_mit-b0.py\",\n",
    "    \"dataset_setting.py\",\n",
    "    \"mmsegmentation/configs/_base_/default_runtime.py\"\n",
    "]\n",
    "\n",
    "data_preprocessor = dict(\n",
    "    type='SegDataPreProcessor',\n",
    "    mean=[0., 0., 0.],\n",
    "    std=[255., 255., 255.],\n",
    "    bgr_to_rgb=True,\n",
    "    size=(512, 512),\n",
    "    pad_val=0,\n",
    "    seg_pad_val=255,\n",
    ")\n",
    "\n",
    "checkpoint=\"https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b3_20220624-13b1141c.pth\"\n",
    "model = dict(\n",
    "    type='EncoderDecoderWithoutArgmax',\n",
    "    data_preprocessor=data_preprocessor,\n",
    "    backbone=dict(\n",
    "        init_cfg=dict(type='Pretrained', checkpoint=checkpoint),\n",
    "        embed_dims=64,\n",
    "        num_heads=[1, 2, 5, 8],\n",
    "        num_layers=[3, 4, 18, 3]),\n",
    "    decode_head=dict(\n",
    "        type='SegformerHeadWithoutAccuracy',\n",
    "        in_channels=[64, 128, 320, 512],\n",
    "        num_classes=29,\n",
    "        loss_decode=dict(\n",
    "            type='CrossEntropyLoss',\n",
    "            use_sigmoid=True,\n",
    "            loss_weight=1.0,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "optimizer = dict(type='AdamW', lr=0.00006, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "optim_wrapper = dict(type='OptimWrapper', optimizer=optimizer, clip_grad=None)\n",
    "\n",
    "# mixed precision\n",
    "fp16 = dict(loss_scale='dynamic')\n",
    "\n",
    "# learning policy\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='LinearLR', start_factor=1e-6, by_epoch=False, begin=0, end=1500\n",
    "    ),\n",
    "    dict(\n",
    "        type='PolyLR',\n",
    "        eta_min=0.0,\n",
    "        power=1.0,\n",
    "        begin=1500,\n",
    "        end=20000,\n",
    "        by_epoch=False,\n",
    "    )\n",
    "]\n",
    "# training schedule for 20k\n",
    "train_cfg = dict(type='IterBasedTrainLoop', max_iters=20000, val_interval=2000)\n",
    "val_cfg = dict(type='ValLoop')\n",
    "test_cfg = dict(type='TestLoop')\n",
    "default_hooks = dict(\n",
    "    timer=dict(type='IterTimerHook'),\n",
    "    logger=dict(type='LoggerHook', interval=50, log_metric_by_epoch=False),\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "    checkpoint=dict(type='CheckpointHook', by_epoch=False, interval=2000),\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "    visualization=dict(type='SegVisualizationHook')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a66e1",
   "metadata": {
    "id": "903a66e1"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22df21c-eb24-4188-ac08-8e0e388dfe3d",
   "metadata": {
    "id": "d22df21c-eb24-4188-ac08-8e0e388dfe3d"
   },
   "outputs": [],
   "source": [
    "# load config\n",
    "cfg = Config.fromfile(\"./config_for_this_example.py\")\n",
    "cfg.launcher = \"none\"\n",
    "cfg.work_dir = \"baseline2\"\n",
    "\n",
    "# resume training\n",
    "cfg.resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ea76f",
   "metadata": {
    "id": "ca2ea76f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner = Runner.from_cfg(cfg)\n",
    "\n",
    "# start training\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bf304",
   "metadata": {
    "id": "2b2bf304"
   },
   "source": [
    "# Result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92658f10",
   "metadata": {
    "id": "92658f10"
   },
   "outputs": [],
   "source": [
    "model = MODELS.build(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c1633",
   "metadata": {
    "id": "d49c1633"
   },
   "outputs": [],
   "source": [
    "checkpoint = load_checkpoint(\n",
    "    model,\n",
    "    \"baseline2/iter_20000.pth\",\n",
    "    map_location='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755b958",
   "metadata": {
    "id": "e755b958"
   },
   "outputs": [],
   "source": [
    "def _preprare_data(imgs, model):\n",
    "    for t in cfg.test_pipeline:\n",
    "        if t.get('type') in ['LoadXRayAnnotations', 'TransposeAnnotations']:\n",
    "            cfg.test_pipeline.remove(t)\n",
    "\n",
    "    is_batch = True\n",
    "    if not isinstance(imgs, (list, tuple)):\n",
    "        imgs = [imgs]\n",
    "        is_batch = False\n",
    "\n",
    "    if isinstance(imgs[0], np.ndarray):\n",
    "        cfg.test_pipeline[0]['type'] = 'LoadImageFromNDArray'\n",
    "\n",
    "    # TODO: Consider using the singleton pattern to avoid building\n",
    "    # a pipeline for each inference\n",
    "    pipeline = Compose(cfg.test_pipeline)\n",
    "\n",
    "    data = defaultdict(list)\n",
    "    for img in imgs:\n",
    "        if isinstance(img, np.ndarray):\n",
    "            data_ = dict(img=img)\n",
    "        else:\n",
    "            data_ = dict(img_path=img)\n",
    "        data_ = pipeline(data_)\n",
    "        data['inputs'].append(data_['inputs'])\n",
    "        data['data_samples'].append(data_['data_samples'])\n",
    "\n",
    "    return data, is_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167e89a",
   "metadata": {
    "id": "d167e89a"
   },
   "outputs": [],
   "source": [
    "# define colors\n",
    "PALETTE = [\n",
    "    (220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230), (106, 0, 228),\n",
    "    (0, 60, 100), (0, 80, 100), (0, 0, 70), (0, 0, 192), (250, 170, 30),\n",
    "    (100, 170, 30), (220, 220, 0), (175, 116, 175), (250, 0, 30), (165, 42, 42),\n",
    "    (255, 77, 255), (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157),\n",
    "    (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118), (255, 179, 240),\n",
    "    (0, 125, 92), (209, 0, 151), (188, 208, 182), (0, 220, 176),\n",
    "]\n",
    "\n",
    "# utility function\n",
    "# this does not care overlap\n",
    "def label2rgb(label):\n",
    "    image_size = label.shape[1:] + (3, )\n",
    "    image = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "    for i, class_label in enumerate(label):\n",
    "        image[class_label == 1] = PALETTE[i]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69365e5d",
   "metadata": {
    "id": "69365e5d"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join(IMAGE_ROOT, \"ID001\", \"image1661130828152_R.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa951e",
   "metadata": {
    "id": "05fa951e"
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "data, is_batch = _preprare_data(img, model)\n",
    "\n",
    "# forward the model\n",
    "with torch.no_grad():\n",
    "    results = model.test_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4c899",
   "metadata": {
    "id": "8ff4c899",
    "outputId": "d92bb48d-4cc4-4aec-ac2e-3aa2ae33ed4b"
   },
   "outputs": [],
   "source": [
    "plt.imshow(label2rgb(results[0].pred_sem_seg.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0502bc-870e-4477-9cff-c044dc81c15b",
   "metadata": {
    "id": "cc0502bc-870e-4477-9cff-c044dc81c15b"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfa4ba-b798-461a-8488-68d75cb74afa",
   "metadata": {
    "id": "27dfa4ba-b798-461a-8488-68d75cb74afa"
   },
   "outputs": [],
   "source": [
    "# 데이터 경로를 입력하세요\n",
    "\n",
    "IMAGE_ROOT = \"/data/ephemeral/home/data/test/DCM/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617a66d-6ca9-4827-8d19-85eccebdaad5",
   "metadata": {
    "id": "8617a66d-6ca9-4827-8d19-85eccebdaad5"
   },
   "outputs": [],
   "source": [
    "pngs = {\n",
    "    os.path.relpath(os.path.join(root, fname), start=IMAGE_ROOT)\n",
    "    for root, _dirs, files in os.walk(IMAGE_ROOT)\n",
    "    for fname in files\n",
    "    if os.path.splitext(fname)[1].lower() == \".png\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62610ea-8fa1-422c-8558-c005692a93b8",
   "metadata": {
    "id": "c62610ea-8fa1-422c-8558-c005692a93b8"
   },
   "outputs": [],
   "source": [
    "def encode_mask_to_rle(mask):\n",
    "    '''\n",
    "    mask: numpy array binary mask\n",
    "    1 - mask\n",
    "    0 - background\n",
    "    Returns encoded run length\n",
    "    '''\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0922758-c8bd-4c08-9e15-391eff2818ad",
   "metadata": {
    "id": "a0922758-c8bd-4c08-9e15-391eff2818ad"
   },
   "outputs": [],
   "source": [
    "def decode_rle_to_mask(rle, height, width):\n",
    "    s = rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(height * width, dtype=np.uint8)\n",
    "\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "\n",
    "    return img.reshape(height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e9b47-61e0-4f2b-ac9d-2a68f5777e52",
   "metadata": {
    "id": "a51e9b47-61e0-4f2b-ac9d-2a68f5777e52"
   },
   "outputs": [],
   "source": [
    "def test(model, image_paths, thr=0.5):\n",
    "    rles = []\n",
    "    filename_and_class = []\n",
    "    with torch.no_grad():\n",
    "        n_class = len(CLASSES)\n",
    "\n",
    "        for step, image_path in tqdm(enumerate(image_paths), total=len(image_paths)):\n",
    "            img = cv2.imread(os.path.join(IMAGE_ROOT,image_path))\n",
    "\n",
    "            # prepare data\n",
    "            data, is_batch = _preprare_data(img, model)\n",
    "\n",
    "            # forward the model\n",
    "            with torch.no_grad():\n",
    "                outputs = model.test_step(data)\n",
    "\n",
    "            outputs = outputs[0].pred_sem_seg.data\n",
    "            outputs = outputs[None]\n",
    "\n",
    "            # restore original size\n",
    "            outputs = F.interpolate(outputs, size=(2048, 2048), mode=\"bilinear\")\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            outputs = (outputs > thr).detach().cpu().numpy()\n",
    "\n",
    "            output = outputs[0]\n",
    "            image_name = os.path.basename(image_path)\n",
    "            for c, segm in enumerate(output):\n",
    "                rle = encode_mask_to_rle(segm)\n",
    "                rles.append(rle)\n",
    "                filename_and_class.append(f\"{IND2CLASS[c]}_{image_name}\")\n",
    "\n",
    "    return rles, filename_and_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90392d7f-5b5e-4aca-8e43-f893b2bb0372",
   "metadata": {
    "id": "90392d7f-5b5e-4aca-8e43-f893b2bb0372"
   },
   "outputs": [],
   "source": [
    "rles, filename_and_class = test(model, pngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0b3f9-0680-40bc-af4a-15cf1fcecb40",
   "metadata": {
    "id": "62b0b3f9-0680-40bc-af4a-15cf1fcecb40"
   },
   "outputs": [],
   "source": [
    "classes, filename = zip(*[x.split(\"_\") for x in filename_and_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6458bf-afb5-4e88-af8a-d47429ede88e",
   "metadata": {
    "id": "2c6458bf-afb5-4e88-af8a-d47429ede88e"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"image_name\": filename,\n",
    "    \"class\": classes,\n",
    "    \"rle\": rles,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d48b6f-58a7-485d-bbaa-8ef76898e99c",
   "metadata": {
    "id": "94d48b6f-58a7-485d-bbaa-8ef76898e99c"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e2c68-31c7-474d-a454-fac3cb73d349",
   "metadata": {
    "id": "db9e2c68-31c7-474d-a454-fac3cb73d349"
   },
   "outputs": [],
   "source": [
    "df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ecf5e8-01de-4ece-8015-45fc97d631cf",
   "metadata": {
    "id": "a3ecf5e8-01de-4ece-8015-45fc97d631cf"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
